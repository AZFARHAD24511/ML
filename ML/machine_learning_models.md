# ðŸ“Š Financial Forecasting Models Evaluation

## ðŸ§  Algorithms Used

In this project, we evaluated the performance of several machine learning algorithms for predicting **log-transformed quarterly revenues** of companies in specific SIC2 industries (`20â€“24`). The following models were implemented using a clean pipeline and time-based train-test split to preserve the temporal structure of the data:

### âœ… Models:

* **XGBoost Regressor**
* **Linear Regression (with StandardScaler)**
* **Lasso Regression (LassoCV with scaling)**
* **Ridge Regression (RidgeCV with scaling)**
* **Random Forest Regressor**

All models were evaluated based on the following performance metrics:

## ðŸ“ Evaluation Metrics

* **RMSE (Root Mean Squared Error)** â€“ Sensitive to large errors
* **MAE (Mean Absolute Error)** â€“ Average absolute error
* **MAPE (Mean Absolute Percentage Error)** â€“ Scale-independent metric
* **RÂ² (Coefficient of Determination)** â€“ Measures the proportion of variance explained

Both **test** and **train** RÂ² scores were recorded to check for **overfitting**.

---

## ðŸ“ˆ Results Summary

| Model             | Test RMSE | Test MAE | MAPE      | Test RÂ²     | Train RÂ² |
| ----------------- | --------- | -------- | --------- | ----------- | -------- |
| **XGBoost**       | 22.37 B   | 7.90 B   | 43.94%    | 0.847       | 0.980    |
| **Linear**        | 3.01e+25  | 1.54e+24 | 1.05e+20% | âŒ -2.76e+26 | âŒ -15.86 |
| **Lasso**         | 3.63e+24  | 1.86e+23 | 1.27e+19% | âŒ -4.01e+27 | âŒ -15.97 |
| **Ridge**         | 8.39e+25  | 4.29e+24 | 2.93e+20% | âŒ ...       | âŒ ...    |
| **Random Forest** | 22.98 B   | 8.10 B   | 42.51%    | 0.839       | 0.978    |

---

## ðŸ§ Observations

* â— **Linear, Lasso, and Ridge regressions failed drastically** on this data, likely due to:

  * Highly skewed and non-stationary financial features
  * Presence of non-linear relationships
  * Extreme sensitivity to feature scaling and outliers

* âœ… **XGBoost** and **Random Forest** performed significantly better, with **XGBoost slightly outperforming** Random Forest in both test RÂ² and error metrics.

---

## ðŸ›¡ï¸ Mitigating Overfitting and Data Leakage

We took several steps to reduce the risk of **overfitting** and **data leakage**:

* Used **log1p transformation** of revenues to reduce skewness and improve numerical stability.
* Applied **rolling means and lag features**, ensuring all engineered features rely only on past data.
* Ensured **time-based train-test split** so that no future information leaks into the training phase.
* Avoided using identifiers (`name`, `sic`) as predictors.
* Evaluated **train vs test RÂ²** to detect signs of overfitting. Linear models showed negative RÂ² due to poor fit, while ensemble models retained generalization.

---

## ðŸš€ Future Work: LSTM (Deep Learning)

While tree-based models like XGBoost are powerful, they do not capture **sequential dependencies** between quarterly financial data. Therefore, the next phase of this project will explore **LSTM (Long Short-Term Memory)** networks using **PyTorch**, which are well-suited for time series and financial sequences.

> The implementation of LSTM is currently in progress and will be shared soon in the next version of this repository.

---

ðŸ“Œ **Conclusion**:
Among the models evaluated, **XGBoost** is the best-performing algorithm on this dataset based on its low error metrics and high RÂ². However, to capture richer temporal dynamics, **LSTM** will be the focus going forward.



